{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hRpsy5Zp8Lr"
   },
   "source": [
    "\n",
    "## <font color=red> You should not import any new libraries. Your code should run with python=3.x</font>\n",
    "\n",
    "#### <font color=red>For lab assignment, you will work with two datasets. The trained weights need to be saved and shared with us in a folder called models with the name ./models/{dataset_name}_weights.pkl. Your predict function should load these weights, initialize the DNN and predict the labels.</font>\n",
    "\n",
    "- Your solutions will be auto-graded. Hence we request you to follow the instructions.\n",
    "- Modify the code only between \n",
    "```\n",
    "## TODO\n",
    "## END TODO\n",
    "```\n",
    "- In addition to above changes, you can play with arguments to the functions for generating plots\n",
    "- We will run the auto grading scripts with private test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mBBZWQn3WjsN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9j3in3odIle"
   },
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iaYuScGvdEum"
   },
   "outputs": [],
   "source": [
    "def preprocessing(X):\n",
    "  \"\"\"\n",
    "  Implement Normalization for input image features\n",
    "\n",
    "  Args:\n",
    "  X : numpy array of shape (n_samples, 784)\n",
    "   \n",
    "  Returns:\n",
    "  X_out: numpy array of shape (n_samples, 784) after normalization\n",
    "  \"\"\"\n",
    "  X_out = None\n",
    "  \n",
    "  ## TODO\n",
    "  X_mean = X.mean(axis=0)\n",
    "  X_std = X.std(axis=0)\n",
    "  X_out = (X-X_mean)/X_std\n",
    "  ## END TODO\n",
    "\n",
    "  assert X_out.shape == X.shape\n",
    "\n",
    "  return X_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejROq-52YUol"
   },
   "source": [
    "### Split data into train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "l07sJgZ3XG-N"
   },
   "outputs": [],
   "source": [
    "def split_data(X, Y, train_ratio=0.8):\n",
    "    '''\n",
    "    Split data into train and validation sets\n",
    "    The first floor(train_ratio*n_sample) samples form the train set\n",
    "    and the remaining the test set\n",
    "\n",
    "    Args:\n",
    "    X - numpy array of shape (n_samples, n_features)\n",
    "    Y - numpy array of shape (n_samples, 1)\n",
    "    train_ratio - fraction of samples to be used as training data\n",
    "\n",
    "    Returns:\n",
    "    X_train, Y_train, X_val, Y_val\n",
    "    '''\n",
    "    # Try Normalization and scaling and store it in X_transformed\n",
    "    X_transformed = X\n",
    "\n",
    "    ## TODO\n",
    "    mx= X.max(axis=1).reshape(-1,1)\n",
    "    mn = X.min(axis=1).reshape(-1,1)\n",
    "    X_transformed = (X-mn)/(mx-mn)\n",
    "    ## END TODO\n",
    "\n",
    "    assert X_transformed.shape == X.shape\n",
    "\n",
    "    num_samples = len(X)\n",
    "    indices = np.arange(num_samples)\n",
    "    num_train_samples = math.floor(num_samples * train_ratio)\n",
    "    train_indices = np.random.choice(indices, num_train_samples, replace=False)\n",
    "    val_indices = list(set(indices) - set(train_indices))\n",
    "    X_train, Y_train, X_val, Y_val = X_transformed[train_indices], Y[train_indices], X_transformed[val_indices], Y[val_indices]\n",
    "  \n",
    "    return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FujjCCbMbsu4"
   },
   "source": [
    "#Flatten the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hl8LxP1lAEiN"
   },
   "outputs": [],
   "source": [
    "class FlattenLayer:\n",
    "    '''\n",
    "    This class converts a multi-dimensional into 1-d vector\n",
    "    '''\n",
    "    def __init__(self, input_shape):\n",
    "        '''\n",
    "         Args:\n",
    "          input_shape : Original shape, tuple of ints\n",
    "        '''\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Converts a multi-dimensional into 1-d vector\n",
    "        Args:\n",
    "          input : training data, numpy array of shape (n_samples , self.input_shape)\n",
    "\n",
    "        Returns:\n",
    "          input: training data, numpy array of shape (n_samples , -1)\n",
    "        '''\n",
    "        ## TODO\n",
    "        nsamps= input.shape[0]\n",
    "        inp = input.reshape(nsamps,-1)\n",
    "        #print(inp.shape)\n",
    "        #Modify the return statement to return flattened input\n",
    "        return inp\n",
    "        ## END TODO\n",
    "        \n",
    "    \n",
    "    def backward(self, output_error, learning_rate):\n",
    "        '''\n",
    "        Converts back the passed array to original dimention \n",
    "        Args:\n",
    "        output_error :  numpy array \n",
    "        learning_rate: float\n",
    "\n",
    "        Returns:\n",
    "        output_error: A reshaped numpy array to allow backward pass\n",
    "        '''\n",
    "        ## TODO\n",
    "        nsamps = output_error.shape[0]\n",
    "        out_sh = [nsamps]+ list(self.input_shape)\n",
    "        output_err = output_error.reshape(out_sh)\n",
    "\n",
    "        #Modify the return statement to return reshaped array\n",
    "        return output_err\n",
    "        ## END TODO\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02MOHEdgh7T6"
   },
   "source": [
    "#Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oTrTMpTwtLXd"
   },
   "outputs": [],
   "source": [
    "class FCLayer:\n",
    "    '''\n",
    "    Implements a fully connected layer  \n",
    "    '''\n",
    "    def __init__(self, input_size, output_size):\n",
    "        '''\n",
    "        Args:\n",
    "         input_size : Input shape, int\n",
    "         output_size: Output shape, int \n",
    "        '''\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        ## TODO\n",
    "        self.weights = np.random.randn(self.input_size, self.output_size)*(1/self.input_size) #initilaise weights for this layer\n",
    "        self.bias = np.random.randn(self.output_size) #initilaise bias for this layer\n",
    "        ## END TODO\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Performs a forward pass of a fully connected network\n",
    "        Args:\n",
    "          input : training data, numpy array of shape (n_samples , self.input_size)\n",
    "\n",
    "        Returns:\n",
    "           numpy array of shape (n_samples , self.output_size)\n",
    "        '''\n",
    "        ## TODO\n",
    "\n",
    "        #Modify the return statement to return numpy array of shape (n_samples , self.output_size)\n",
    "        self.oldZ= np.copy(input)\n",
    "        Zval = np.dot(input, self.weights) + self.bias\n",
    "        return Zval\n",
    "        ## END TODO\n",
    "        \n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        '''\n",
    "        Performs a backward pass of a fully connected network along with updating the parameter \n",
    "        Args:\n",
    "          output_error :  numpy array \n",
    "          learning_rate: float\n",
    "\n",
    "        Returns:\n",
    "          Numpy array resulting from the backward pass\n",
    "        '''\n",
    "        ## TODO\n",
    "        #out_error is nsamps , output_size\n",
    "        #res should nsamps, input_size\n",
    "        nsamps = output_error.shape[0]\n",
    "        dW = (1/nsamps)*np.matmul(self.oldZ.T,output_error)\n",
    "        db = np.mean(output_error, axis=0)\n",
    "        dA = np.matmul(output_error, self.weights.T)\n",
    "        \n",
    "        self.weights -= learning_rate*dW\n",
    "        self.bias -= learning_rate*db\n",
    "        \n",
    "\n",
    "        #Modify the return statement to return numpy array resulting from backward pass\n",
    "        return dA\n",
    "        ## END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "E6nSYAB2sam3"
   },
   "outputs": [],
   "source": [
    "class ActivationLayer:\n",
    "    '''\n",
    "    Implements a Activation layer which applies activation function on the inputs. \n",
    "    '''\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        '''\n",
    "          Args:\n",
    "          activation : Name of the activation function (sigmoid,tanh or relu)\n",
    "          activation_prime: Name of the corresponding function to be used during backpropagation (sigmoid_prime,tanh_prime or relu_prime)\n",
    "        '''\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "    \n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Applies the activation function \n",
    "        Args:\n",
    "          input : numpy array on which activation function is to be applied\n",
    "\n",
    "        Returns:\n",
    "           numpy array output from the activation function\n",
    "        '''\n",
    "        ## TODO\n",
    "        self.Zv = self.activation(input)\n",
    "        \n",
    "        #Modify the return statement to return numpy array of shape (n_samples , self.output_size)\n",
    "        return self.Zv\n",
    "        ## END TODO\n",
    "        \n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        '''\n",
    "        Performs a backward pass of a fully connected network along with updating the parameter \n",
    "        Args:\n",
    "          output_error :  numpy array \n",
    "          learning_rate: float\n",
    "\n",
    "        Returns:\n",
    "          Numpy array resulting from the backward pass\n",
    "        '''\n",
    "        ## TODO\n",
    "        outss = (learning_rate*output_error)*self.activation_prime(self.Zv)\n",
    "        #Modify the return statement to return numpy array resulting from backward pass\n",
    "        return outss\n",
    "        ## END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RQeuIfkK3vyl"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SoftmaxLayer:\n",
    "    '''\n",
    "      Implements a Softmax layer which applies softmax function on the inputs. \n",
    "    '''\n",
    "    def __init__(self, input_size):\n",
    "        self.input_size = input_size\n",
    "    \n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Applies the softmax function \n",
    "        Args:\n",
    "          input : numpy array on which softmax function is to be applied\n",
    "\n",
    "        Returns:\n",
    "           numpy array output from the softmax function\n",
    "        '''\n",
    "        ## TODO\n",
    "        exps = np.exp(input)\n",
    "        sums = np.sum(exps, axis =1).reshape(-1,1)\n",
    "        self.res = exps/sums\n",
    "        \n",
    "        #Modify the return statement to return numpy array of shape (n_samples , self.output_size)\n",
    "        return self.res\n",
    "        ## END TODO\n",
    "        \n",
    "    def backward(self, output_error, learning_rate):\n",
    "        '''\n",
    "        Performs a backward pass of a Softmax layer\n",
    "        Args:\n",
    "          output_error :  numpy array \n",
    "          learning_rate: float\n",
    "\n",
    "        Returns:\n",
    "          Numpy array resulting from the backward pass\n",
    "        '''\n",
    "        ## TODO\n",
    "\n",
    "        #Modify the return statement to return numpy array resulting from backward pass\n",
    "        return self.res * (output_error -(output_error * self.res).sum(axis=1)[:,None])\n",
    "        ## END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LuPbn70Wt8Q7"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    Sigmoid function \n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying simoid function\n",
    "    '''\n",
    "    ## TODO\n",
    "    exps = np.exp(x)\n",
    "    res = exps/(1. + exps)\n",
    "    #Modify the return statement to return numpy array resulting from backward pass\n",
    "    return res\n",
    "    ## END TODO\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    '''\n",
    "     Implements derivative of Sigmoid function, for the backward pass\n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying derivative of Sigmoid function\n",
    "    '''\n",
    "    ## TODO\n",
    "    \n",
    "    #Modify the return statement to return numpy array resulting from backward pass\n",
    "    return x*(1-x)\n",
    "    ## END TODO\n",
    "\n",
    "def tanh(x):\n",
    "    '''\n",
    "    Tanh function \n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying tanh function\n",
    "    '''\n",
    "    ## TODO\n",
    "    tanhx=(np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "    #Modify the return statement to return numpy array resulting from backward pass\n",
    "    return tanhx\n",
    "    ## END TODO\n",
    "\n",
    "def tanh_prime(x):\n",
    "    '''\n",
    "     Implements derivative of Tanh function, for the backward pass\n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying derivative of Tanh function\n",
    "    '''\n",
    "    ## TODO\n",
    "    tanhx=(np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "    #Modify the return statement to return numpy array resulting from backward pass\n",
    "    return 1 - tanhx**2\n",
    "    ## END TODO\n",
    "\n",
    "def relu(x):\n",
    "    '''\n",
    "    ReLU function \n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying ReLU function\n",
    "    '''\n",
    "    ## TODO\n",
    "\n",
    "    #Modify the return statement to return numpy array resulting from backward pass\n",
    "    return np.clip(x,0, None)\n",
    "    ## END TODO\n",
    "\n",
    "def relu_prime(x):\n",
    "    '''\n",
    "     Implements derivative of ReLU function, for the backward pass\n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying derivative of ReLU function\n",
    "    '''\n",
    "    ## TODO\n",
    "\n",
    "    #Modify the return statement to return numpy array resulting from backward pass\n",
    "    return np.where(x>0,1,0)\n",
    "    ## END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rXY7jkUzuqEk"
   },
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    '''\n",
    "    MSE loss\n",
    "    Args:\n",
    "        y_true :  Ground truth labels, numpy array \n",
    "        y_true :  Predicted labels, numpy array \n",
    "    Returns:\n",
    "       loss : float\n",
    "    '''\n",
    "    ## TODO\n",
    "\n",
    "    #Modify the return statement to return numpy array resulting from backward pass\n",
    "    return np.mean(np.square(y_pred - y_true), axis=1)\n",
    "    ## END TODO\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    '''\n",
    "    Implements derivative of MSE function, for the backward pass\n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying derivative of MSE function\n",
    "    '''\n",
    "    ## TODO\n",
    "\n",
    "    #Modify the return statement to return numpy array resulting from backward pass\n",
    "    #print(y_true.shape)\n",
    "    return 2*(1/y_true.shape[1])*(y_pred-y_true)\n",
    "    ## END TODO\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    '''\n",
    "    Cross entropy loss \n",
    "    Args:\n",
    "        y_true :  Ground truth labels, numpy array \n",
    "        y_true :  Predicted labels, numpy array \n",
    "    Returns:\n",
    "       loss : float\n",
    "    '''\n",
    "    ## TODO\n",
    "\n",
    "    #Modify the return statement to return numpy array resulting from backward pass\n",
    "    \n",
    "    return (np.where(y_true==1, -np.log(np.clip(y_pred, 1e-8, None)), 0)).sum(axis=1)\n",
    "    ## END TODO\n",
    "\n",
    "def cross_entropy_prime(y_true, y_pred):\n",
    "    '''\n",
    "    Implements derivative of cross entropy function, for the backward pass\n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying derivative of cross entropy function\n",
    "    '''\n",
    "    ## TODO\n",
    "\n",
    "    #Modify the return statement to return numpy array resulting from backward pass\n",
    "    return np.where(y_true==1, -1/np.clip(y_pred, 1e-8, None), 0)\n",
    "    ## END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u23euUDztNtb"
   },
   "source": [
    "Fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-sCYdGN8tSdp"
   },
   "outputs": [],
   "source": [
    "# def fit(X_train, Y_train,dataset_name):\n",
    "\n",
    "#     '''\n",
    "#     Create and trains a feedforward network\n",
    "\n",
    "#     Do not forget to save the final weights of the feed forward network to a file. Use these weights in the `predict` function \n",
    "#     Args:\n",
    "#         X_train -- np array of share (num_test, 2048) for flowers and (num_test, 28, 28) for mnist.\n",
    "#         Y_train -- np array of share (num_test, 2048) for flowers and (num_test, 28, 28) for mnist.\n",
    "#         dataset_name -- name of the dataset (flowers or mnist)\n",
    "    \n",
    "#     '''\n",
    "     \n",
    "#     #Note that this just a template to help you create your own feed forward network \n",
    "#     ## TODO\n",
    "\n",
    "#     #define your network\n",
    "#     #This network would work only for mnist\n",
    "#     network = [\n",
    "#         FlattenLayer(input_shape=(28, 28)),\n",
    "#         FCLayer(28 * 28, 12),\n",
    "#         ActivationLayer(sigmoid, sigmoid_prime),\n",
    "#         FCLayer(12, 10),\n",
    "#         SoftmaxLayer(10)\n",
    "#     ] # This creates feed forward \n",
    "\n",
    "\n",
    "#     # Choose appropriate learning rate and no. of epoch\n",
    "#     epochs = 40\n",
    "#     learning_rate = 0.01\n",
    "\n",
    "#     # Change training loop as you see fit\n",
    "#     for epoch in range(epochs):\n",
    "#         error = 0\n",
    "#         i=0\n",
    "#         for x, y_true in zip(X_train, Y_train):\n",
    "#             # forward\n",
    "            \n",
    "#             output = x.reshape(1,28,28)\n",
    "#             for layer in network:\n",
    "#                 output = layer.forward(output)\n",
    "            \n",
    "#             # error (display purpose only)\n",
    "#             #print(y_true)\n",
    "#             y_true = np.array([y_true])\n",
    "#             y_tru = np.zeros((y_true.size, 10))\n",
    "#             y_tru[np.arange(y_true.size), y_true]=1\n",
    "#             error += mse(y_tru, output)\n",
    "\n",
    "#             # backward\n",
    "#             output_error = mse_prime(y_tru, output)\n",
    "#             for layer in reversed(network):\n",
    "#                 output_error = layer.backward(output_error, learning_rate)\n",
    "#             i+=1\n",
    "        \n",
    "#         error /= len(X_train)\n",
    "#         print('%d/%d, error=%f' % (epoch + 1, epochs, error))\n",
    "\n",
    "#     #Save you model weights\n",
    "    \n",
    "#     ## END TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X_train, Y_train,dataset_name):\n",
    "\n",
    "    '''\n",
    "    Create and trains a feedforward network\n",
    "\n",
    "    Do not forget to save the final weights of the feed forward network to a file. Use these weights in the `predict` function \n",
    "    Args:\n",
    "        X_train -- np array of share (num_test, 2048) for flowers and (num_test, 28, 28) for mnist.\n",
    "        Y_train -- np array of share (num_test, 2048) for flowers and (num_test, 28, 28) for mnist.\n",
    "        dataset_name -- name of the dataset (flowers or mnist)\n",
    "    \n",
    "    '''\n",
    "     \n",
    "    #Note that this just a template to help you create your own feed forward network \n",
    "    ## TODO\n",
    "    \n",
    "    # Choose appropriate learning rate and no. of epoch\n",
    "    index = ['mnist', 'flowers'].index(dataset_name)\n",
    "\n",
    "    input_shape = X_train.shape[1:]\n",
    "    epochs = [100, 500][index]\n",
    "    learning_rate = 0.01\n",
    "    batch_size = [20, 8][index]\n",
    "    n_labels = [10, 5][index]\n",
    "    \n",
    "    # nonrmalize and store in X_norm\n",
    "    mu, sigma = np.mean(X_train, axis=0), np.std(X_train, axis=0)\n",
    "    norm_pms = [mu, sigma]\n",
    "    sigma[sigma == 0] = 1\n",
    "    X_norm = (X_train - mu)/(sigma)\n",
    "    \n",
    "    # scale X_norm and store in X_scaled\n",
    "    diff = (np.max(X_norm, axis=0) - np.min(X_norm, axis=0))\n",
    "    diff[diff == 0] = 1\n",
    "    X_scaled = (X_norm - np.min(X_norm, axis=0)) / (diff)\n",
    "    \n",
    "    # update X_train \n",
    "    X_train = X_scaled   \n",
    "    \n",
    "    #define your network\n",
    "    #This network would work only for mnist\n",
    "    network = [\n",
    "        FlattenLayer(input_shape=input_shape),\n",
    "        FCLayer(np.prod(input_shape), 12),\n",
    "        ActivationLayer(sigmoid, sigmoid_prime),\n",
    "        FCLayer(12, n_labels),\n",
    "        SoftmaxLayer(n_labels)\n",
    "    ] # This creates feed forward \n",
    "\n",
    "\n",
    "    # Change training loop as you see fit\n",
    "    ls = []\n",
    "    for epoch in range(epochs):\n",
    "        error = 0\n",
    "        for i in range(0, X_train.shape[0], batch_size):\n",
    "\n",
    "            output = X_train[i:i + batch_size]\n",
    "            for layer in network:\n",
    "                output = layer.forward(output)\n",
    "\n",
    "            y_true = Y_train[i:i + batch_size]\n",
    "\n",
    "            y_vec = np.zeros((batch_size, n_labels))\n",
    "            for j in range(batch_size):\n",
    "                y_vec[j, y_true[j]] = 1\n",
    "\n",
    "            error += cross_entropy(y_vec, output).sum()\n",
    "#             print(y_vec.shape, output.shape)\n",
    "\n",
    "            output_error = cross_entropy_prime(y_vec, output)\n",
    "            for layer in reversed(network):\n",
    "                output_error = layer.backward(output_error, learning_rate)\n",
    "\n",
    "        error /= len(X_train)\n",
    "        ls.append(error)\n",
    "        print('%d/%d, error=%f' % (epoch + 1, epochs, error))\n",
    "\n",
    "    # Save you model weights\n",
    "    pkl.dump([norm_pms, network], open(f\"./models/{dataset_name}_weights.pkl\", \"wb\"))\n",
    "    return ls\n",
    "    \n",
    "    ## END TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3Pop_HsvuEZ"
   },
   "source": [
    "Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ttYbN2psvtu_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x -- (60000, 28, 28); train_y -- (60000,)\n",
      "1/100, error=2.307008\n",
      "2/100, error=2.297431\n",
      "3/100, error=2.292846\n",
      "4/100, error=2.284933\n",
      "5/100, error=2.270323\n",
      "6/100, error=2.243177\n",
      "7/100, error=2.195428\n",
      "8/100, error=2.120664\n",
      "9/100, error=2.019711\n",
      "10/100, error=1.899549\n",
      "11/100, error=1.768259\n",
      "12/100, error=1.634811\n",
      "13/100, error=1.508413\n",
      "14/100, error=1.395429\n",
      "15/100, error=1.298377\n",
      "16/100, error=1.216613\n",
      "17/100, error=1.147528\n",
      "18/100, error=1.087945\n",
      "19/100, error=1.035065\n",
      "20/100, error=0.986830\n",
      "21/100, error=0.941932\n",
      "22/100, error=0.899699\n",
      "23/100, error=0.859931\n",
      "24/100, error=0.822714\n",
      "25/100, error=0.788235\n",
      "26/100, error=0.756637\n",
      "27/100, error=0.727934\n",
      "28/100, error=0.702005\n",
      "29/100, error=0.678629\n",
      "30/100, error=0.657538\n",
      "31/100, error=0.638452\n",
      "32/100, error=0.621110\n",
      "33/100, error=0.605279\n",
      "34/100, error=0.590757\n",
      "35/100, error=0.577375\n",
      "36/100, error=0.564989\n",
      "37/100, error=0.553481\n",
      "38/100, error=0.542751\n",
      "39/100, error=0.532716\n",
      "40/100, error=0.523306\n",
      "41/100, error=0.514460\n",
      "42/100, error=0.506128\n",
      "43/100, error=0.498265\n",
      "44/100, error=0.490833\n",
      "45/100, error=0.483798\n",
      "46/100, error=0.477129\n",
      "47/100, error=0.470800\n",
      "48/100, error=0.464787\n",
      "49/100, error=0.459068\n",
      "50/100, error=0.453623\n",
      "51/100, error=0.448434\n",
      "52/100, error=0.443485\n",
      "53/100, error=0.438759\n",
      "54/100, error=0.434244\n",
      "55/100, error=0.429925\n",
      "56/100, error=0.425792\n",
      "57/100, error=0.421833\n",
      "58/100, error=0.418037\n",
      "59/100, error=0.414395\n",
      "60/100, error=0.410898\n",
      "61/100, error=0.407538\n",
      "62/100, error=0.404307\n",
      "63/100, error=0.401198\n",
      "64/100, error=0.398204\n",
      "65/100, error=0.395318\n",
      "66/100, error=0.392536\n",
      "67/100, error=0.389851\n",
      "68/100, error=0.387258\n",
      "69/100, error=0.384753\n",
      "70/100, error=0.382331\n",
      "71/100, error=0.379988\n",
      "72/100, error=0.377720\n",
      "73/100, error=0.375523\n",
      "74/100, error=0.373394\n",
      "75/100, error=0.371329\n",
      "76/100, error=0.369327\n",
      "77/100, error=0.367383\n",
      "78/100, error=0.365495\n",
      "79/100, error=0.363660\n",
      "80/100, error=0.361877\n",
      "81/100, error=0.360142\n",
      "82/100, error=0.358455\n",
      "83/100, error=0.356812\n",
      "84/100, error=0.355213\n",
      "85/100, error=0.353654\n",
      "86/100, error=0.352135\n",
      "87/100, error=0.350654\n",
      "88/100, error=0.349210\n",
      "89/100, error=0.347800\n",
      "90/100, error=0.346425\n",
      "91/100, error=0.345082\n",
      "92/100, error=0.343770\n",
      "93/100, error=0.342488\n",
      "94/100, error=0.341235\n",
      "95/100, error=0.340010\n",
      "96/100, error=0.338812\n",
      "97/100, error=0.337641\n",
      "98/100, error=0.336494\n",
      "99/100, error=0.335372\n",
      "100/100, error=0.334273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.3070080309604943,\n",
       " 2.2974313098162193,\n",
       " 2.2928459238856918,\n",
       " 2.2849330502865928,\n",
       " 2.270322876380689,\n",
       " 2.243176798647577,\n",
       " 2.195428306081371,\n",
       " 2.1206636134988917,\n",
       " 2.0197112195345266,\n",
       " 1.8995491292742166,\n",
       " 1.7682586235581144,\n",
       " 1.634810668205768,\n",
       " 1.5084132321412707,\n",
       " 1.3954286931356703,\n",
       " 1.298376859311883,\n",
       " 1.2166128774175098,\n",
       " 1.1475284761902245,\n",
       " 1.0879449659790308,\n",
       " 1.035065003785546,\n",
       " 0.9868297252003673,\n",
       " 0.941932031698256,\n",
       " 0.8996994751420686,\n",
       " 0.8599313347852371,\n",
       " 0.8227138010027113,\n",
       " 0.7882351486414082,\n",
       " 0.7566370144133316,\n",
       " 0.7279339583606007,\n",
       " 0.7020047324731621,\n",
       " 0.6786292223395467,\n",
       " 0.6575378567772124,\n",
       " 0.6384522114756729,\n",
       " 0.6211103557350387,\n",
       " 0.6052790322460266,\n",
       " 0.590757170334133,\n",
       " 0.5773746484951725,\n",
       " 0.5649889095512087,\n",
       " 0.5534809169293716,\n",
       " 0.5427512064341374,\n",
       " 0.5327163626425203,\n",
       " 0.5233060232962942,\n",
       " 0.5144604048854041,\n",
       " 0.5061282951062037,\n",
       " 0.4982654425591059,\n",
       " 0.490833274237129,\n",
       " 0.4837978782465611,\n",
       " 0.4771291983233866,\n",
       " 0.4708003959162591,\n",
       " 0.46478734396280313,\n",
       " 0.459068223668563,\n",
       " 0.4536232015783699,\n",
       " 0.4484341691090345,\n",
       " 0.44348453063664367,\n",
       " 0.4387590293542294,\n",
       " 0.4342436025779558,\n",
       " 0.42992526010666227,\n",
       " 0.42579198073623864,\n",
       " 0.42183262318483933,\n",
       " 0.4180368485699607,\n",
       " 0.4143950522525429,\n",
       " 0.41089830337407324,\n",
       " 0.407538290797732,\n",
       " 0.4043072744539912,\n",
       " 0.40119804130784287,\n",
       " 0.39820386532730595,\n",
       " 0.39531847095476763,\n",
       " 0.3925359996745199,\n",
       " 0.3898509793397512,\n",
       " 0.38725829597562694,\n",
       " 0.3847531678168797,\n",
       " 0.38233112137109404,\n",
       " 0.37998796932531964,\n",
       " 0.3777197901352495,\n",
       " 0.37552290915413483,\n",
       " 0.3733938811737146,\n",
       " 0.3713294742625027,\n",
       " 0.36932665479785204,\n",
       " 0.3673825735981051,\n",
       " 0.3654945530696933,\n",
       " 0.363660075291658,\n",
       " 0.36187677096682247,\n",
       " 0.36014240917486734,\n",
       " 0.35845488786793883,\n",
       " 0.3568122250542541,\n",
       " 0.3552125506194895,\n",
       " 0.3536540987396809,\n",
       " 0.3521352008428767,\n",
       " 0.35065427908000707,\n",
       " 0.3492098402683578,\n",
       " 0.3478004702736701,\n",
       " 0.34642482879935826,\n",
       " 0.3450816445535644,\n",
       " 0.3437697107667844,\n",
       " 0.3424878810347596,\n",
       " 0.34123506546304644,\n",
       " 0.3400102270913055,\n",
       " 0.3388123785768587,\n",
       " 0.33764057911851036,\n",
       " 0.3364939316028748,\n",
       " 0.3353715799567434,\n",
       " 0.33427270669012943]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = \"mnist\" \n",
    "with open(f\"./data/{dataset}_train.pkl\", \"rb\") as file:\n",
    "    train_mnist = pkl.load(file)\n",
    "    print(f\"train_x -- {train_mnist[0].shape}; train_y -- {train_mnist[1].shape}\")\n",
    "\n",
    "fit(train_mnist[0],train_mnist[1],'mnist')\n",
    "\n",
    "# dataset = \"flowers\" # \"mnist\"/\"flowers\"\n",
    "# with open(f\"./data/{dataset}_train.pkl\", \"rb\") as file:\n",
    "#     train_flowers = pkl.load(file)\n",
    "#     print(f\"train_x -- {train_flowers[0].shape}; train_y -- {train_flowers[1].shape}\")\n",
    "\n",
    "# fit(train_flowers[0],train_flowers[1],'flowers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "QprSHht4iwe9"
   },
   "outputs": [],
   "source": [
    "def predict(X_test, dataset_name):\n",
    "  \"\"\"\n",
    "\n",
    "  X_test -- np array of share (num_test, 2048) for flowers and (num_test, 28, 28) for mnist.\n",
    "\n",
    "   \n",
    "\n",
    "  This is the only function that we will call from the auto grader. \n",
    "\n",
    "  This function should only perform inference, please donot train your models here.\n",
    "  \n",
    "  Steps to be done here:\n",
    "  1. Load your trained weights from ./models/{dataset_name}_weights.pkl\n",
    "  2. Ensure that you read weights using only the libraries we have given above.\n",
    "  3. Initialize your model with your trained weights\n",
    "  4. Compute the predicted labels and return it\n",
    "\n",
    "  Please provide us the complete code you used for training including any techniques\n",
    "  like data augmentation etc. that you have tried out. \n",
    "\n",
    "  Return:\n",
    "  Y_test - nparray of shape (num_test,)\n",
    "  \"\"\"\n",
    "  Y_test = np.zeros(X_test.shape[0],)\n",
    "\n",
    "  ## TODO\n",
    "    \n",
    "  norm_pms, network = pkl.load(open(f'./models/{dataset_name}_weights.pkl', 'rb'))\n",
    "    \n",
    "  # nonrmalize and store in X_norm\n",
    "  mu, sigma = norm_pms\n",
    "  sigma[sigma == 0] = 1\n",
    "  X_norm = (X_test - mu)/(sigma)\n",
    "    \n",
    "  # scale X_norm and store in X_scaled\n",
    "  diff = (np.max(X_norm, axis=0) - np.min(X_norm, axis=0))\n",
    "  diff[diff == 0] = 1\n",
    "  X_scaled = (X_norm - np.min(X_norm, axis=0)) / (diff)\n",
    "    \n",
    "  # update X_train \n",
    "  output = X_scaled\n",
    "    \n",
    "  for layer in network:\n",
    "    output = layer.forward(output)\n",
    "    \n",
    "  Y_test = np.argmax(output, axis=1)\n",
    "  ## END TODO\n",
    "  assert Y_test.shape == (X_test.shape[0],) and type(Y_test) == type(X_test), \"Check what you return\"\n",
    "  return Y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x -- (60000, 28, 28); train_y -- (60000,)\n",
      "\n",
      "\n",
      "MNIST Dataset - Training Results\n",
      "\n",
      "Precision: 0.9060170347851025\n",
      "Recall: 0.9059270037374121\n",
      "F1-score: 0.9058814718458276\n",
      "Accuracy: 0.9071\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = \"mnist\" \n",
    "with open(f\"./data/{dataset}_train.pkl\", \"rb\") as file:\n",
    "    train_mnist = pkl.load(file)\n",
    "    print(f\"train_x -- {train_mnist[0].shape}; train_y -- {train_mnist[1].shape}\")\n",
    "    X_test = train_mnist[0]\n",
    "    Y_test = train_mnist[1]\n",
    "    \n",
    "Y_pred = predict(X_test, dataset)\n",
    "acc = np.sum(Y_pred == Y_test)/Y_test.shape[0]\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "\n",
    "p, r, f, _ =precision_recall_fscore_support(Y_test, Y_pred, average='macro')\n",
    "report={'Precision':p, 'Recall':r, 'F1-score':f, \"Accuracy\":acc}\n",
    "\n",
    "print(\"\\n\\nMNIST Dataset - Training Results\\n\")\n",
    "for i in report:\n",
    "    print(\"{}: {}\".format(i, report[i]))\n",
    "    \n",
    "print(\"\\n\\n\\n\")\n",
    "# df = pd.DataFrame(report)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
