{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hRpsy5Zp8Lr"
   },
   "source": [
    "\n",
    "## <font color=red> You should not import any new libraries. Your code should run with python=3.x</font>\n",
    "\n",
    "#### <font color=red>For lab assignment, you will work with two datasets. The trained weights need to be saved and shared with us in a folder called models with the name ./models/{dataset_name}_weights.pkl. Your predict function should load these weights, initialize the DNN and predict the labels.</font>\n",
    "\n",
    "- Your solutions will be auto-graded. Hence we request you to follow the instructions.\n",
    "- Modify the code only between \n",
    "```\n",
    "## TODO\n",
    "## END TODO\n",
    "```\n",
    "- In addition to above changes, you can play with arguments to the functions for generating plots\n",
    "- We will run the auto grading scripts with private test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mBBZWQn3WjsN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9j3in3odIle"
   },
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iaYuScGvdEum"
   },
   "outputs": [],
   "source": [
    "def preprocessing(X):\n",
    "  \"\"\"\n",
    "  Implement Normalization for input image features\n",
    "\n",
    "  Args:\n",
    "  X : numpy array of shape (n_samples, 784)\n",
    "   \n",
    "  Returns:\n",
    "  X_out: numpy array of shape (n_samples, 784) after normalization\n",
    "  \"\"\"\n",
    "  X_out = None\n",
    "  \n",
    "  ## TODO\n",
    "    \n",
    "  mu = np.mean(X, axis=0)\n",
    "  sigma = np.std(axis=0)\n",
    "  X_out = (X-mu)/sigma\n",
    "  \n",
    "  ## END TODO\n",
    "\n",
    "  assert X_out.shape == X.shape\n",
    "\n",
    "  return X_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejROq-52YUol"
   },
   "source": [
    "### Split data into train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "l07sJgZ3XG-N"
   },
   "outputs": [],
   "source": [
    "def split_data(X, Y, train_ratio=0.8):\n",
    "    '''\n",
    "    Split data into train and validation sets\n",
    "    The first floor(train_ratio*n_sample) samples form the train set\n",
    "    and the remaining the test set\n",
    "\n",
    "    Args:\n",
    "    X - numpy array of shape (n_samples, n_features)\n",
    "    Y - numpy array of shape (n_samples, 1)\n",
    "    train_ratio - fraction of samples to be used as training data\n",
    "\n",
    "    Returns:\n",
    "    X_train, Y_train, X_val, Y_val\n",
    "    '''\n",
    "    # Try Normalization and scaling and store it in X_transformed\n",
    "    X_transformed = X\n",
    "\n",
    "    ## TODO\n",
    "    \n",
    "    M = np.max(X, axis=1).reshape(-1,1)\n",
    "    m = np.min(X, axis=1).reshape(-1,1)\n",
    "    X_transformed = (X-m)/(M-m)\n",
    "    \n",
    "    ## END TODO\n",
    "\n",
    "    assert X_transformed.shape == X.shape\n",
    "\n",
    "    num_samples = len(X)\n",
    "    indices = np.arange(num_samples)\n",
    "    num_train_samples = math.floor(num_samples * train_ratio)\n",
    "    train_indices = np.random.choice(indices, num_train_samples, replace=False)\n",
    "    val_indices = list(set(indices) - set(train_indices))\n",
    "    X_train, Y_train, X_val, Y_val = X_transformed[train_indices], Y[train_indices], X_transformed[val_indices], Y[val_indices]\n",
    "  \n",
    "    return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FujjCCbMbsu4"
   },
   "source": [
    "#Flatten the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hl8LxP1lAEiN"
   },
   "outputs": [],
   "source": [
    "class FlattenLayer:\n",
    "    '''\n",
    "    This class converts a multi-dimensional into 1-d vector\n",
    "    '''\n",
    "    def __init__(self, input_shape):\n",
    "        '''\n",
    "         Args:\n",
    "          input_shape : Original shape, tuple of ints\n",
    "        '''\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Converts a multi-dimensional into 1-d vector\n",
    "        Args:\n",
    "          input : training data, numpy array of shape (n_samples , self.input_shape)\n",
    "\n",
    "        Returns:\n",
    "          input: training data, numpy array of shape (n_samples , -1)\n",
    "        '''\n",
    "        ## TODO\n",
    "        n_samp = input.shape[0]\n",
    "        inp = input.reshape(n_samp, -1)\n",
    "        #Modify the return statement to return flattened input\n",
    "        return inp\n",
    "        ## END TODO\n",
    "        \n",
    "    \n",
    "    def backward(self, output_error, learning_rate):\n",
    "        '''\n",
    "        Converts back the passed array to original dimention \n",
    "        Args:\n",
    "        output_error :  numpy array \n",
    "        learning_rate: float\n",
    "\n",
    "        Returns:\n",
    "        output_error: A reshaped numpy array to allow backward pass\n",
    "        '''\n",
    "        ## TODO\n",
    "        nsamps = output_error.shape[0]\n",
    "        out_sh = [nsamps]+ list(self.input_shape)\n",
    "        output_err = output_error.reshape(out_sh)\n",
    "        #Modify the return statement to return reshaped array\n",
    "        return output_error\n",
    "        ## END TODO\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02MOHEdgh7T6"
   },
   "source": [
    "#Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oTrTMpTwtLXd"
   },
   "outputs": [],
   "source": [
    "class FCLayer:\n",
    "    '''\n",
    "    Implements a fully connected layer  \n",
    "    '''\n",
    "    def __init__(self, input_size, output_size):\n",
    "        '''\n",
    "        Args:\n",
    "         input_size : Input shape, int\n",
    "         output_size: Output shape, int \n",
    "        '''\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        ## TODO\n",
    "        self.weights = np.random.randn(self.input_size, self.output_size)*(1/self.input_size) #initilaise weights for this layer\n",
    "        self.bias = np.random.randn(self.output_size) #initilaise bias for this layer\n",
    "        ## END TODO\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Performs a forward pass of a fully connected network\n",
    "        Args:\n",
    "          input : training data, numpy array of shape (n_samples , self.input_size)\n",
    "\n",
    "        Returns:\n",
    "           numpy array of shape (n_samples , self.output_size)\n",
    "        '''\n",
    "        ## TODO\n",
    "        self.Z0 = np.copy(input)\n",
    "        Z1 = np.dot(input, self.weights) + self.bias\n",
    "        #Modify the return statement to return numpy array of shape (n_samples , self.output_size)\n",
    "        return Z1\n",
    "        ## END TODO\n",
    "        \n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        '''\n",
    "        Performs a backward pass of a fully connected network along with updating the parameter \n",
    "        Args:\n",
    "          output_error :  numpy array \n",
    "          learning_rate: float\n",
    "\n",
    "        Returns:\n",
    "          Numpy array resulting from the backward pass\n",
    "        '''\n",
    "        ## TODO\n",
    "        \n",
    "        nsamps = output_error.shape[0]\n",
    "        dW = (1/nsamps)*np.matmul(self.Z0.T, output_error)\n",
    "        db = np.mean(output_error, axis=0)\n",
    "        dA = np.matmul(output_error, self.weights.T)\n",
    "        \n",
    "        self.weights -= learning_rate*dW\n",
    "        self.bias -= learning_rate*db\n",
    "\n",
    "        #Modify the return statement to return numpy array resulting from backward pass\n",
    "        return dA\n",
    "        ## END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "E6nSYAB2sam3"
   },
   "outputs": [],
   "source": [
    "class ActivationLayer:\n",
    "    '''\n",
    "    Implements a Activation layer which applies activation function on the inputs. \n",
    "    '''\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        '''\n",
    "          Args:\n",
    "          activation : Name of the activation function (sigmoid,tanh or relu)\n",
    "          activation_prime: Name of the corresponding function to be used during backpropagation (sigmoid_prime,tanh_prime or relu_prime)\n",
    "        '''\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "    \n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Applies the activation function \n",
    "        Args:\n",
    "          input : numpy array on which activation function is to be applied\n",
    "\n",
    "        Returns:\n",
    "           numpy array output from the activation function\n",
    "        '''\n",
    "        ## TODO\n",
    "        self.Z = self.activation(input)\n",
    "        #Modify the return statement to return numpy array of shape (n_samples , self.output_size)\n",
    "        return self.Z\n",
    "        ## END TODO\n",
    "        \n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        '''\n",
    "        Performs a backward pass of a fully connected network along with updating the parameter \n",
    "        Args:\n",
    "          output_error :  numpy array \n",
    "          learning_rate: float\n",
    "\n",
    "        Returns:\n",
    "          Numpy array resulting from the backward pass\n",
    "        '''\n",
    "        ## TODO\n",
    "        out = (learning_rate * output_error) * self.activation_prime(self.Z)\n",
    "        #Modify the return statement to return numpy array resulting from backward pass\n",
    "        return out\n",
    "        ## END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RQeuIfkK3vyl"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SoftmaxLayer:\n",
    "    '''\n",
    "      Implements a Softmax layer which applies softmax function on the inputs. \n",
    "    '''\n",
    "    def __init__(self, input_size):\n",
    "        self.input_size = input_size\n",
    "    \n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Applies the softmax function \n",
    "        Args:\n",
    "          input : numpy array on which softmax function is to be applied\n",
    "\n",
    "        Returns:\n",
    "           numpy array output from the softmax function\n",
    "        '''\n",
    "        ## TODO\n",
    "        e1 = np.exp(input)\n",
    "        esum = np.sum(e1, axis =1).reshape(-1,1)\n",
    "        self.out = e1/esum\n",
    "        #Modify the return statement to return numpy array of shape (n_samples , self.output_size)\n",
    "        return self.out\n",
    "        ## END TODO\n",
    "        \n",
    "    def backward(self, output_error, learning_rate):\n",
    "        '''\n",
    "        Performs a backward pass of a Softmax layer\n",
    "        Args:\n",
    "          output_error :  numpy array \n",
    "          learning_rate: float\n",
    "\n",
    "        Returns:\n",
    "          Numpy array resulting from the backward pass\n",
    "        '''\n",
    "        ## TODO\n",
    "        do = self.out * (output_error -(output_error * self.out).sum(axis=1)[:,None])\n",
    "        #Modify the return statement to return numpy array resulting from backward pass\n",
    "        return do\n",
    "        ## END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LuPbn70Wt8Q7"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    Sigmoid function \n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying simoid function\n",
    "    '''\n",
    "    ## TODO\n",
    "    z = 1./(1. + np.exp(-x))\n",
    "    #Modify the return statement to return numpy array resulting from backward pass\n",
    "    return z\n",
    "    ## END TODO\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    '''\n",
    "     Implements derivative of Sigmoid function, for the backward pass\n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying derivative of Sigmoid function\n",
    "    '''\n",
    "    ## TODO\n",
    "    dz = x*(1. -x)\n",
    "    #Modify the return statement to return numpy array resulting from backward pass\n",
    "    return dz\n",
    "    ## END TODO\n",
    "\n",
    "def tanh(x):\n",
    "    '''\n",
    "    Tanh function \n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying tanh function\n",
    "    '''\n",
    "    ## TODO\n",
    "    z = (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "    #Modify the return statement to return numpy array resulting from backward pass\n",
    "    return z\n",
    "    ## END TODO\n",
    "\n",
    "def tanh_prime(x):\n",
    "    '''\n",
    "     Implements derivative of Tanh function, for the backward pass\n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying derivative of Tanh function\n",
    "    '''\n",
    "    ## TODO\n",
    "    dz = (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "    #Modify the return statement to return numpy array resulting from backward pass\n",
    "    return dz\n",
    "    ## END TODO\n",
    "\n",
    "def relu(x):\n",
    "    '''\n",
    "    ReLU function \n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying ReLU function\n",
    "    '''\n",
    "    ## TODO\n",
    "    z = x * (x > 0)\n",
    "    #Modify the return statement to return numpy array resulting from backward pass\n",
    "    return z\n",
    "    ## END TODO\n",
    "\n",
    "def relu_prime(x):\n",
    "    '''\n",
    "     Implements derivative of ReLU function, for the backward pass\n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying derivative of ReLU function\n",
    "    '''\n",
    "    ## TODO\n",
    "    dz = 1. * (x > 0)\n",
    "    #Modify the return statement to return numpy array resulting from backward pass\n",
    "    return dz\n",
    "    ## END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rXY7jkUzuqEk"
   },
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    '''\n",
    "    MSE loss\n",
    "    Args:\n",
    "        y_true :  Ground truth labels, numpy array \n",
    "        y_true :  Predicted labels, numpy array \n",
    "    Returns:\n",
    "       loss : float\n",
    "    '''\n",
    "    ## TODO\n",
    "    loss = np.mean(np.square(y_pred - y_true), axis=1)\n",
    "    #Modify the return statement to return numpy array resulting from backward pass\n",
    "    return loss\n",
    "    ## END TODO\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    '''\n",
    "    Implements derivative of MSE function, for the backward pass\n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying derivative of MSE function\n",
    "    '''\n",
    "    ## TODO\n",
    "    dl = 2*(1/y_true.shape[1])*(y_pred-y_true)\n",
    "    #Modify the return statement to return numpy array resulting from backward pass\n",
    "    return dl\n",
    "    ## END TODO\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    '''\n",
    "    Cross entropy loss \n",
    "    Args:\n",
    "        y_true :  Ground truth labels, numpy array \n",
    "        y_true :  Predicted labels, numpy array \n",
    "    Returns:\n",
    "       loss : float\n",
    "    '''\n",
    "    ## TODO\n",
    "    loss = (np.where(y_true==1, -np.log(np.clip(y_pred, 1e-8, None)), 0)).sum(axis=1)\n",
    "    #Modify the return statement to return numpy array resulting from backward pass\n",
    "    return loss\n",
    "    ## END TODO\n",
    "\n",
    "def cross_entropy_prime(y_true, y_pred):\n",
    "    '''\n",
    "    Implements derivative of cross entropy function, for the backward pass\n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying derivative of cross entropy function\n",
    "    '''\n",
    "    ## TODO\n",
    "    dl = np.where(y_true==1, -1/np.clip(y_pred, 1e-8, None), 0)\n",
    "    #Modify the return statement to return numpy array resulting from backward pass\n",
    "    return dl\n",
    "    ## END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u23euUDztNtb"
   },
   "source": [
    "Fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X_train, Y_train,dataset_name):\n",
    "\n",
    "    '''\n",
    "    Create and trains a feedforward network\n",
    "\n",
    "    Do not forget to save the final weights of the feed forward network to a file. Use these weights in the `predict` function \n",
    "    Args:\n",
    "        X_train -- np array of share (num_test, 2048) for flowers and (num_test, 28, 28) for mnist.\n",
    "        Y_train -- np array of share (num_test, 2048) for flowers and (num_test, 28, 28) for mnist.\n",
    "        dataset_name -- name of the dataset (flowers or mnist)\n",
    "    \n",
    "    '''\n",
    "     \n",
    "    #Note that this just a template to help you create your own feed forward network \n",
    "    ## TODO\n",
    "    \n",
    "    # Choose appropriate learning rate and no. of epoch\n",
    "    index = ['mnist', 'flowers'].index(dataset_name)\n",
    "\n",
    "    input_shape = X_train.shape[1:]\n",
    "    epochs = [100, 100][index]\n",
    "    learning_rate = 0.025\n",
    "    batch_size = [16, 8][index]\n",
    "    n_labels = [10, 5][index]\n",
    "    \n",
    "    # nonrmalize and store in X_norm\n",
    "    mu, sigma = np.mean(X_train, axis=0), np.std(X_train, axis=0)\n",
    "    norm_pms = [mu, sigma]\n",
    "    sigma[sigma == 0] = 1\n",
    "    X_norm = (X_train - mu)/(sigma)\n",
    "    \n",
    "    # scale X_norm and store in X_scaled\n",
    "    diff = (np.max(X_norm, axis=0) - np.min(X_norm, axis=0))\n",
    "    diff[diff == 0] = 1\n",
    "    X_scaled = (X_norm - np.min(X_norm, axis=0)) / (diff)\n",
    "    \n",
    "    # update X_train \n",
    "    X_train = X_scaled   \n",
    "    \n",
    "    #define your network\n",
    "    #This network would work only for mnist\n",
    "    \n",
    "    hidden_layer = 12\n",
    "    network = [\n",
    "        FlattenLayer(input_shape=input_shape),\n",
    "        FCLayer(np.prod(input_shape), hidden_layer),\n",
    "        ActivationLayer(sigmoid, sigmoid_prime),\n",
    "        FCLayer(hidden_layer, n_labels),\n",
    "        SoftmaxLayer(n_labels)\n",
    "    ] # This creates feed forward \n",
    "\n",
    "\n",
    "    # Change training loop as you see fit\n",
    "    ls = []\n",
    "    for epoch in range(epochs):\n",
    "        error = 0\n",
    "        for i in range(0, X_train.shape[0], batch_size):\n",
    "\n",
    "            output = X_train[i:i + batch_size]\n",
    "            for layer in network:\n",
    "                output = layer.forward(output)\n",
    "\n",
    "            y_true = Y_train[i:i + batch_size]\n",
    "\n",
    "            y_vec = np.zeros((batch_size, n_labels))\n",
    "            for j in range(batch_size):\n",
    "                y_vec[j, y_true[j]] = 1\n",
    "\n",
    "            error += cross_entropy(y_vec, output).sum()\n",
    "#             print(y_vec.shape, output.shape)\n",
    "\n",
    "            output_error = cross_entropy_prime(y_vec, output)\n",
    "            for layer in reversed(network):\n",
    "                output_error = layer.backward(output_error, learning_rate)\n",
    "\n",
    "        error /= len(X_train)\n",
    "        ls.append(error)\n",
    "        print('%d/%d, error=%f' % (epoch + 1, epochs, error))\n",
    "\n",
    "    # Save you model weights\n",
    "    pkl.dump([norm_pms, network], open(f\"./models/{dataset_name}_weights.pkl\", \"wb\"))\n",
    "    return ls\n",
    "    \n",
    "    ## END TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3Pop_HsvuEZ"
   },
   "source": [
    "Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ttYbN2psvtu_",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x -- (60000, 28, 28); train_y -- (60000,)\n",
      "1/100, error=2.290907\n",
      "2/100, error=2.069346\n",
      "3/100, error=1.423590\n",
      "4/100, error=0.986878\n",
      "5/100, error=0.764528\n",
      "6/100, error=0.639199\n",
      "7/100, error=0.563119\n",
      "8/100, error=0.512448\n",
      "9/100, error=0.476148\n",
      "10/100, error=0.448838\n",
      "11/100, error=0.427544\n",
      "12/100, error=0.410441\n",
      "13/100, error=0.396342\n",
      "14/100, error=0.384451\n",
      "15/100, error=0.374225\n",
      "16/100, error=0.365283\n",
      "17/100, error=0.357360\n",
      "18/100, error=0.350261\n",
      "19/100, error=0.343844\n",
      "20/100, error=0.338002\n",
      "21/100, error=0.332651\n",
      "22/100, error=0.327728\n",
      "23/100, error=0.323179\n",
      "24/100, error=0.318961\n",
      "25/100, error=0.315037\n",
      "26/100, error=0.311376\n",
      "27/100, error=0.307951\n",
      "28/100, error=0.304739\n",
      "29/100, error=0.301719\n",
      "30/100, error=0.298873\n",
      "31/100, error=0.296184\n",
      "32/100, error=0.293639\n",
      "33/100, error=0.291225\n",
      "34/100, error=0.288930\n",
      "35/100, error=0.286744\n",
      "36/100, error=0.284659\n",
      "37/100, error=0.282666\n",
      "38/100, error=0.280757\n",
      "39/100, error=0.278926\n",
      "40/100, error=0.277167\n",
      "41/100, error=0.275475\n",
      "42/100, error=0.273844\n",
      "43/100, error=0.272270\n",
      "44/100, error=0.270749\n",
      "45/100, error=0.269278\n",
      "46/100, error=0.267852\n",
      "47/100, error=0.266468\n",
      "48/100, error=0.265125\n",
      "49/100, error=0.263819\n",
      "50/100, error=0.262547\n",
      "51/100, error=0.261308\n",
      "52/100, error=0.260100\n",
      "53/100, error=0.258920\n",
      "54/100, error=0.257767\n",
      "55/100, error=0.256640\n",
      "56/100, error=0.255537\n",
      "57/100, error=0.254456\n",
      "58/100, error=0.253396\n",
      "59/100, error=0.252357\n",
      "60/100, error=0.251337\n",
      "61/100, error=0.250335\n",
      "62/100, error=0.249351\n",
      "63/100, error=0.248383\n",
      "64/100, error=0.247431\n",
      "65/100, error=0.246494\n",
      "66/100, error=0.245572\n",
      "67/100, error=0.244664\n",
      "68/100, error=0.243769\n",
      "69/100, error=0.242887\n",
      "70/100, error=0.242017\n",
      "71/100, error=0.241160\n",
      "72/100, error=0.240314\n",
      "73/100, error=0.239480\n",
      "74/100, error=0.238657\n",
      "75/100, error=0.237844\n",
      "76/100, error=0.237042\n",
      "77/100, error=0.236251\n",
      "78/100, error=0.235469\n",
      "79/100, error=0.234697\n",
      "80/100, error=0.233935\n",
      "81/100, error=0.233183\n",
      "82/100, error=0.232439\n",
      "83/100, error=0.231705\n",
      "84/100, error=0.230979\n",
      "85/100, error=0.230263\n",
      "86/100, error=0.229555\n",
      "87/100, error=0.228855\n",
      "88/100, error=0.228164\n",
      "89/100, error=0.227481\n",
      "90/100, error=0.226806\n",
      "91/100, error=0.226139\n",
      "92/100, error=0.225480\n",
      "93/100, error=0.224829\n",
      "94/100, error=0.224185\n",
      "95/100, error=0.223549\n",
      "96/100, error=0.222920\n",
      "97/100, error=0.222299\n",
      "98/100, error=0.221685\n",
      "99/100, error=0.221077\n",
      "100/100, error=0.220477\n",
      "train_x -- (2936, 2048); train_y -- (2936,)\n",
      "1/100, error=1.623354\n",
      "2/100, error=1.602432\n",
      "3/100, error=1.601489\n",
      "4/100, error=1.600436\n",
      "5/100, error=1.599217\n",
      "6/100, error=1.597764\n",
      "7/100, error=1.595990\n",
      "8/100, error=1.593785\n",
      "9/100, error=1.591006\n",
      "10/100, error=1.587470\n",
      "11/100, error=1.582940\n",
      "12/100, error=1.577115\n",
      "13/100, error=1.569616\n",
      "14/100, error=1.559975\n",
      "15/100, error=1.547634\n",
      "16/100, error=1.531950\n",
      "17/100, error=1.512240\n",
      "18/100, error=1.487858\n",
      "19/100, error=1.458351\n",
      "20/100, error=1.423676\n",
      "21/100, error=1.384446\n",
      "22/100, error=1.342050\n",
      "23/100, error=1.298470\n",
      "24/100, error=1.255793\n",
      "25/100, error=1.215674\n",
      "26/100, error=1.179058\n",
      "27/100, error=1.146230\n",
      "28/100, error=1.117031\n",
      "29/100, error=1.091072\n",
      "30/100, error=1.067890\n",
      "31/100, error=1.047023\n",
      "32/100, error=1.028054\n",
      "33/100, error=1.010619\n",
      "34/100, error=0.994411\n",
      "35/100, error=0.979171\n",
      "36/100, error=0.964683\n",
      "37/100, error=0.950772\n",
      "38/100, error=0.937295\n",
      "39/100, error=0.924137\n",
      "40/100, error=0.911211\n",
      "41/100, error=0.898452\n",
      "42/100, error=0.885816\n",
      "43/100, error=0.873277\n",
      "44/100, error=0.860824\n",
      "45/100, error=0.848458\n",
      "46/100, error=0.836192\n",
      "47/100, error=0.824044\n",
      "48/100, error=0.812037\n",
      "49/100, error=0.800195\n",
      "50/100, error=0.788542\n",
      "51/100, error=0.777102\n",
      "52/100, error=0.765893\n",
      "53/100, error=0.754932\n",
      "54/100, error=0.744232\n",
      "55/100, error=0.733802\n",
      "56/100, error=0.723646\n",
      "57/100, error=0.713767\n",
      "58/100, error=0.704164\n",
      "59/100, error=0.694834\n",
      "60/100, error=0.685771\n",
      "61/100, error=0.676970\n",
      "62/100, error=0.668421\n",
      "63/100, error=0.660117\n",
      "64/100, error=0.652047\n",
      "65/100, error=0.644202\n",
      "66/100, error=0.636573\n",
      "67/100, error=0.629149\n",
      "68/100, error=0.621920\n",
      "69/100, error=0.614877\n",
      "70/100, error=0.608012\n",
      "71/100, error=0.601315\n",
      "72/100, error=0.594778\n",
      "73/100, error=0.588394\n",
      "74/100, error=0.582156\n",
      "75/100, error=0.576056\n",
      "76/100, error=0.570089\n",
      "77/100, error=0.564249\n",
      "78/100, error=0.558530\n",
      "79/100, error=0.552928\n",
      "80/100, error=0.547438\n",
      "81/100, error=0.542056\n",
      "82/100, error=0.536778\n",
      "83/100, error=0.531600\n",
      "84/100, error=0.526520\n",
      "85/100, error=0.521535\n",
      "86/100, error=0.516641\n",
      "87/100, error=0.511835\n",
      "88/100, error=0.507117\n",
      "89/100, error=0.502482\n",
      "90/100, error=0.497930\n",
      "91/100, error=0.493458\n",
      "92/100, error=0.489063\n",
      "93/100, error=0.484745\n",
      "94/100, error=0.480502\n",
      "95/100, error=0.476331\n",
      "96/100, error=0.472231\n",
      "97/100, error=0.468201\n",
      "98/100, error=0.464239\n",
      "99/100, error=0.460343\n",
      "100/100, error=0.456513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6233542928941298,\n",
       " 1.602432367164825,\n",
       " 1.6014886543167723,\n",
       " 1.600435688979745,\n",
       " 1.5992171533208994,\n",
       " 1.5977641529042552,\n",
       " 1.5959902155133383,\n",
       " 1.5937849011340026,\n",
       " 1.5910059280528106,\n",
       " 1.5874696215740571,\n",
       " 1.5829396923668795,\n",
       " 1.57711474763127,\n",
       " 1.5696156502345195,\n",
       " 1.5599750629893017,\n",
       " 1.547633574158827,\n",
       " 1.5319501835736558,\n",
       " 1.5122400323394452,\n",
       " 1.4878582465019594,\n",
       " 1.4583506045449892,\n",
       " 1.4236755259788096,\n",
       " 1.384446207391988,\n",
       " 1.342050150904955,\n",
       " 1.2984699224142435,\n",
       " 1.255793028289091,\n",
       " 1.215673821257149,\n",
       " 1.1790577864075686,\n",
       " 1.1462300984168512,\n",
       " 1.1170308136104394,\n",
       " 1.0910723470999004,\n",
       " 1.0678896027510012,\n",
       " 1.0470225900586905,\n",
       " 1.0280536256294017,\n",
       " 1.010619470468825,\n",
       " 0.9944114984344239,\n",
       " 0.979171047242201,\n",
       " 0.9646834607243931,\n",
       " 0.9507723654805678,\n",
       " 0.9372947354405012,\n",
       " 0.92413682210399,\n",
       " 0.9112108113036003,\n",
       " 0.8984519816514543,\n",
       " 0.8858161263188088,\n",
       " 0.8732770323020043,\n",
       " 0.8608238760894817,\n",
       " 0.8484584790331585,\n",
       " 0.8361924536820594,\n",
       " 0.8240443453704095,\n",
       " 0.8120369154574738,\n",
       " 0.8001947159957373,\n",
       " 0.7885420737303187,\n",
       " 0.7771015473246937,\n",
       " 0.7658928628149018,\n",
       " 0.7549322834450445,\n",
       " 0.7442323395390714,\n",
       " 0.7338018328495938,\n",
       " 0.7236460335713872,\n",
       " 0.713767000554828,\n",
       " 0.7041639704749808,\n",
       " 0.6948337760539791,\n",
       " 0.6857712652201531,\n",
       " 0.6769697020996455,\n",
       " 0.6684211374558626,\n",
       " 0.6601167412297664,\n",
       " 0.6520470936566257,\n",
       " 0.6442024343171427,\n",
       " 0.6365728705695722,\n",
       " 0.6291485482038994,\n",
       " 0.6219197879628869,\n",
       " 0.614877191910084,\n",
       " 0.608011723623424,\n",
       " 0.6013147659786022,\n",
       " 0.5947781599608148,\n",
       " 0.5883942275799091,\n",
       " 0.5821557816080069,\n",
       " 0.5760561245329767,\n",
       " 0.5700890388329104,\n",
       " 0.5642487704238585,\n",
       " 0.5585300069094067,\n",
       " 0.5529278520591321,\n",
       " 0.5474377977578609,\n",
       " 0.5420556944946273,\n",
       " 0.536777721297259,\n",
       " 0.5316003558645904,\n",
       " 0.5265203455040086,\n",
       " 0.521534679348054,\n",
       " 0.5166405622016543,\n",
       " 0.5118353902621084,\n",
       " 0.5071167288583578,\n",
       " 0.502482292274657,\n",
       " 0.4979299256566168,\n",
       " 0.4934575889442701,\n",
       " 0.4890633427363653,\n",
       " 0.4847453359613612,\n",
       " 0.4805017952120571,\n",
       " 0.47633101559084096,\n",
       " 0.4722313529095759,\n",
       " 0.4682012170905746,\n",
       " 0.4642390666215455,\n",
       " 0.4603434039265129,\n",
       " 0.45651277152559844]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = \"mnist\" \n",
    "with open(f\"./data/{dataset}_train.pkl\", \"rb\") as file:\n",
    "    train_mnist = pkl.load(file)\n",
    "    print(f\"train_x -- {train_mnist[0].shape}; train_y -- {train_mnist[1].shape}\")\n",
    "\n",
    "fit(train_mnist[0],train_mnist[1],'mnist')\n",
    "\n",
    "dataset = \"flowers\" # \"mnist\"/\"flowers\"\n",
    "with open(f\"./data/{dataset}_train.pkl\", \"rb\") as file:\n",
    "    train_flowers = pkl.load(file)\n",
    "    print(f\"train_x -- {train_flowers[0].shape}; train_y -- {train_flowers[1].shape}\")\n",
    "\n",
    "fit(train_flowers[0],train_flowers[1],'flowers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "QprSHht4iwe9"
   },
   "outputs": [],
   "source": [
    "def predict(X_test, dataset_name):\n",
    "  \"\"\"\n",
    "\n",
    "  X_test -- np array of share (num_test, 2048) for flowers and (num_test, 28, 28) for mnist.\n",
    "\n",
    "   \n",
    "\n",
    "  This is the only function that we will call from the auto grader. \n",
    "\n",
    "  This function should only perform inference, please donot train your models here.\n",
    "  \n",
    "  Steps to be done here:\n",
    "  1. Load your trained weights from ./models/{dataset_name}_weights.pkl\n",
    "  2. Ensure that you read weights using only the libraries we have given above.\n",
    "  3. Initialize your model with your trained weights\n",
    "  4. Compute the predicted labels and return it\n",
    "\n",
    "  Please provide us the complete code you used for training including any techniques\n",
    "  like data augmentation etc. that you have tried out. \n",
    "\n",
    "  Return:\n",
    "  Y_test - nparray of shape (num_test,)\n",
    "  \"\"\"\n",
    "  Y_test = np.zeros(X_test.shape[0],)\n",
    "\n",
    "  ## TODO\n",
    "    \n",
    "  norm_pms, network = pkl.load(open(f'./models/{dataset_name}_weights.pkl', 'rb'))\n",
    "    \n",
    "  # nonrmalize and store in X_norm\n",
    "  mu, sigma = norm_pms\n",
    "  sigma[sigma == 0] = 1\n",
    "  X_norm = (X_test - mu)/(sigma)\n",
    "    \n",
    "  # scale X_norm and store in X_scaled\n",
    "  diff = (np.max(X_norm, axis=0) - np.min(X_norm, axis=0))\n",
    "  diff[diff == 0] = 1\n",
    "  X_scaled = (X_norm - np.min(X_norm, axis=0)) / (diff)\n",
    "    \n",
    "  # update X_train \n",
    "  output = X_scaled\n",
    "    \n",
    "  for layer in network:\n",
    "    output = layer.forward(output)\n",
    "    \n",
    "  Y_test = np.argmax(output, axis=1)\n",
    "  ## END TODO\n",
    "  assert Y_test.shape == (X_test.shape[0],) and type(Y_test) == type(X_test), \"Check what you return\"\n",
    "  return Y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
