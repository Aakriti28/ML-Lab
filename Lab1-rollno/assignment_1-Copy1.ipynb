{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8gKZpl5me4I"
   },
   "source": [
    "\n",
    "## <font color=red> You should not import any new libraries. Your code should run with python=3.x</font>\n",
    "## <font color=red> Please don't rename this .ipynb file.</font><br>\n",
    "- Your solutions will be auto-graded. Hence we request you to follow the instructions.\n",
    "- Modify the code only between \n",
    "```\n",
    "## TODO\n",
    "## END TODO\n",
    "```\n",
    "- In addition to above changes, you can play with arguments to the functions for generating plots\n",
    "- We will run the auto grading scripts with private test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gnXVhbxMc10V"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from numpy.linalg import inv # use this function to invert matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tzeArL4rODu"
   },
   "source": [
    "## Please make sure that your code works with loading data from relative path only\n",
    "\n",
    "i.e. ```pd.read_csv('./data/single_var_new.csv')``` should not throw an error when we run the auto-grading scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8dO3U5A1rODu"
   },
   "outputs": [],
   "source": [
    "data_single = pd.read_csv('./data/single_var.csv')\n",
    "X_single = np.array(data_single['x_gt'])\n",
    "Y_single = np.array(data_single['y_gt'])\n",
    "\n",
    "data_multi = pd.read_csv('./data/multi_var.csv')\n",
    "cols = [f\"x_gt_{idx}\" for idx in range(1, 6)]\n",
    "X_multi = np.array(data_multi[cols])\n",
    "Y_multi = np.array(data_multi['y_gt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ko_aLCmDdXvZ"
   },
   "source": [
    "## Plot Graphs\n",
    "\n",
    "- This function plots the ground truth curve in <font color=green>green</font> and the predicted function in <font color=red>red</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xlXKQnMtrODv"
   },
   "outputs": [],
   "source": [
    "def plot_curves(w, b, x, y):\n",
    "  '''\n",
    "  Plots the curves for groud truth function and the fitted function\n",
    "\n",
    "  Args:\n",
    "  w - list of parameters\n",
    "  b - float\n",
    "  x - data features\n",
    "  y - output value\n",
    "  '''\n",
    "\n",
    "  assert type(w) == type([0]), f'Please pass a list of parameters to plot_curves and not {type(w)}'\n",
    "\n",
    "  x_gt = np.linspace(-1, 2, 50)\n",
    "  y_gt = 1 - 3 * x_gt - 2 * x_gt ** 2 + 2.5 * x_gt ** 3\n",
    "\n",
    "  if len(w) == 1:\n",
    "#     print(w, b)\n",
    "    y_fit = w * x_gt + b\n",
    "  elif len(w) == 5:\n",
    "    x_fit = x_gt\n",
    "    for pow in range(2, 4):\n",
    "      x_fit = np.vstack([x_fit, np.power(x_gt, pow)])\n",
    "    \n",
    "    x_fit = np.vstack([x_fit, np.sin(np.pi * 2 * x_gt)])\n",
    "    x_fit = np.vstack([x_fit, np.cos(np.pi * x_gt)])\n",
    "    y_fit = np.dot(w, x_fit) + b\n",
    "  else:\n",
    "    assert False, 'Pass a valid w'\n",
    "  plt.plot(x_gt, y_gt, color=\"green\", label='1 - 3 * x - 2 * x ** 2 + 2.5 * x ** 3')\n",
    "#   print(y_fit)\n",
    "  plt.plot(x_gt, y_fit, color='red', label=\"Fitted Function y = w.Tx + b\")\n",
    "  if len(x.shape) == 1:\n",
    "    x_plot = np.vstack([x, np.ones(len(x))]).T\n",
    "  else:\n",
    "    x_plot = x\n",
    "  plt.scatter(x_plot[:,0],y)\n",
    "  plt.legend()\n",
    "  plt.title(\"OLS\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2x12RnAoy45s"
   },
   "source": [
    "## Split data into train/validation\n",
    "- make sure that training and validation datasets are disjoint\n",
    "- Split data into train and validation sets such that train contains floor(n_sampes * train_ratio) and test contains the remaining samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wq4DFSUprODw"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-046f308af183>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-046f308af183>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    rng = np.random.default_rng(seed=)\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def split_data(X, Y, train_ratio=0.6):\n",
    "    '''\n",
    "    Split data into train and validation sets such that train\n",
    "    contains floor(n_sampes * train_ratio) and test contains the remaining\n",
    "    samples\n",
    "\n",
    "    Args:\n",
    "    X - numpy array of shape (n_samples, n_features)\n",
    "    Y - numpy array of shape (n_samples, 1)\n",
    "    train_ratio - fraction of samples to be used as training data\n",
    "\n",
    "    Returns:\n",
    "    X_train, Y_train, X_val, Y_val\n",
    "    '''\n",
    "\n",
    "    ## TODO\n",
    "    split_val = int(len(Y)*train_ratio)\n",
    "    rng = np.random.default_rng(seed=)\n",
    "    arr = np.arange(len(Y))\n",
    "    rng.shuffle(arr)\n",
    "    X = X[arr]\n",
    "    Y = Y[arr]\n",
    "    X_train = X[:split_val]\n",
    "    Y_train = Y[:split_val]\n",
    "    X_val = X[split_val:]\n",
    "    Y_val = Y[split_val:]\n",
    "    \n",
    "    ## END TODO\n",
    "    \n",
    "    return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_single.shape, Y_single.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Eh-aXuzrODy"
   },
   "source": [
    "## mse for single variable regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbLHCBUyrODz"
   },
   "outputs": [],
   "source": [
    "def mse_single_var(X, Y, w, b):\n",
    "    '''\n",
    "    Compute mean squared error between predictions and true y values\n",
    "\n",
    "    Args:\n",
    "    X - numpy array of shape (n_samples, 1)\n",
    "    Y - numpy array of shape (n_samples, 1)\n",
    "    w - a float\n",
    "    b - a float\n",
    "    '''\n",
    "\n",
    "    ## TODO\n",
    "#     print('mse-single', np.square(w*X + b - Y).shape)\n",
    "    mse = np.mean(np.square(w*X + b - Y))\n",
    "#     mse2 = 0\n",
    "#     for i in range(len(Y)):\n",
    "#         mse2 += (w*X[i] + b - Y[i])**2\n",
    "#     mse2 /= len(Y)\n",
    "    \n",
    "#     print('mse-single', mse, mse2)\n",
    "#     mse2 = 1/len(Y) * np.sum((w*X + b - Y)**2)\n",
    "#     print(mse, mse2)\n",
    "#     if mse == mse2:\n",
    "#         print(True)\n",
    "#     else:\n",
    "#         print(False)\n",
    "\n",
    "    ## END TODO\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 2, 3, 4, 5])\n",
    "print(np.mean(np.square(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rYAAHfL7rODz"
   },
   "outputs": [],
   "source": [
    "def mse_multi_var(X, Y, w, b):\n",
    "    '''\n",
    "    Compute mean squared error between predictions and true y values\n",
    "\n",
    "    Args:\n",
    "    X - numpy array of shape (n_samples, 5)\n",
    "    Y - numpy array of shape (n_samples, 1)\n",
    "    w - list of parameters\n",
    "    b - a float\n",
    "    '''\n",
    "\n",
    "    ## TODO\n",
    "    \n",
    "#     print(np.dot(X, w).shape, w.shape)\n",
    "    w = np.array(w)\n",
    "    y_hat = np.matmul(X, w) + b\n",
    "#     print('aaaaaaa', y_hat.shape, Y.shape)\n",
    "    mse = np.mean(np.square(y_hat - Y))\n",
    "    \n",
    "#     mse2 = 0\n",
    "#     for i in range(len(Y)):\n",
    "#         mse2 += (w@X[i, :] + b - Y[i])**2\n",
    "#     mse2 /= len(Y)\n",
    "#     print('mse-multi', mse, mse2)\n",
    "    \n",
    "    ## END TODO\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Opc-jPt8rODz"
   },
   "outputs": [],
   "source": [
    "def mse_regularized(X, Y, w, b, lamda):\n",
    "    '''\n",
    "    Compute mean squared error between predictions and true y values\n",
    "\n",
    "    Args:\n",
    "    X - numpy array of shape (n_samples, 5)\n",
    "    Y - numpy array of shape (n_samples, 1)\n",
    "    w - list of parameters\n",
    "    b - a float\n",
    "    '''\n",
    "\n",
    "    ## TODO\n",
    "    w = np.array(w)\n",
    "    print('mse-reg', np.square(w).shape)\n",
    "    mse = np.mean(np.square(np.matmul(X, w) + b - Y)) + lamda*np.sum(np.square(w))\n",
    "\n",
    "    ## END TODO\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyeQbbkjdE6v"
   },
   "source": [
    "# Single Variable Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ws8XUJXJrOD0"
   },
   "outputs": [],
   "source": [
    "def singlevar_grad(X_train, Y_train, X_val, Y_val, epochs=100, lr=1e-3):\n",
    "    '''\n",
    "    Perform single variable least squares regression using gradient descent\n",
    "\n",
    "    Args:\n",
    "    X_train - numpy array of shape (n_samples_train, 1)\n",
    "    Y_train - numpy array of shape (n_samples_train, 1)\n",
    "    X_val - numpy array of shape (n_samples_val, 1)\n",
    "    Y_val - numpy array of shape (n_samples_val, 1)\n",
    "    epochs - number of gradient descent steps\n",
    "    lr - learnig rate\n",
    "    '''\n",
    "\n",
    "    # Initialize the parameters with 0\n",
    "    w = 0\n",
    "    b = 0\n",
    "\n",
    "    ## TODO\n",
    "    n = len(Y_train)\n",
    "    for i in range(epochs):\n",
    "        y_pred = w*X_train + b\n",
    "#         print(np.dot((y_pred - Y_train), X_train).shape)\n",
    "#         print((y_pred - Y_train).shape)\n",
    "        dw = 2*(np.matmul((y_pred - Y_train), X_train))/n\n",
    "        db = 2*np.sum(y_pred - Y_train)/n\n",
    "        w -= lr*dw\n",
    "        b -= lr*db\n",
    "        \n",
    "#         print(w, b)\n",
    "    ## END TODO\n",
    "\n",
    "    mse_train = mse_single_var(X_train, Y_train, w, b)\n",
    "    mse_val = mse_single_var(X_val, Y_val, w, b)\n",
    "    print(f'Validation loss is {mse_val}')\n",
    "    print(f'Training Loss loss is {mse_train}')\n",
    "    plot_curves([w], b, X_train, Y_train)\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def singlevar_closedform(X_train, Y_train, X_val, Y_val):\n",
    "    '''\n",
    "    Perform single variable least squares regression using closed form update \n",
    "    rules\n",
    "\n",
    "    Args:\n",
    "    X_train - numpy array of shape (n_samples_train, 1)\n",
    "    Y_train - numpy array of shape (n_samples_train, 1)\n",
    "    X_val - numpy array of shape (n_samples_val, 1)\n",
    "    Y_val - numpy array of shape (n_samples_val, 1)\n",
    "    '''\n",
    "\n",
    "    w = 0\n",
    "    b = 0\n",
    "\n",
    "    ## TODO\n",
    "    w_b = np.concatenate(([w], [b]), axis=0)\n",
    "#     print(w_b.shape)\n",
    "    \n",
    "    X = X_train.reshape(X_train.shape[0], 1)\n",
    "    X = np.hstack((X, np.ones((X_train.shape[0], 1), dtype=X_train.dtype)))\n",
    "#     print(X.shape, X_train.shape, Y_train.shape)\n",
    "\n",
    "    w_b = np.dot(inv(np.dot(X.T, X)), np.dot(X.T, Y_train))\n",
    "    w = w_b[0]\n",
    "    b = w_b[-1]\n",
    "#     print(w, b)\n",
    "    \n",
    "#     w = (np.dot(X_train, X_train))*np.dot(X_train, Y_train)\n",
    "#     X = np.concatenate((X_train, X_val), axis=0)\n",
    "#     Y = np.concatenate((Y_train, Y_val), axis=0)\n",
    "#     print(np.dot(X.T, Y).shape)\n",
    "#     w = np.dot(X_train.T, Y_train)/np.dot(X_train.T, X_train)\n",
    "    \n",
    "    ## END TODO\n",
    "\n",
    "\n",
    "    mse_train = mse_single_var(X_train, Y_train, w, b)\n",
    "    mse_val = mse_single_var(X_val, Y_val, w, b)\n",
    "    print(f'Validation loss is {mse_val}')\n",
    "    print(f'Training Loss loss is {mse_train}')\n",
    "    plot_curves([w], b, X_train, Y_train)\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gD47NfTcrqed"
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_val, Y_val = split_data(X_single, Y_single, train_ratio=0.6)\n",
    "\n",
    "singlevar_grad(X_train, Y_train, X_val, Y_val, lr=0.01)\n",
    "\n",
    "print('----'*30)\n",
    "\n",
    "singlevar_closedform(X_train, Y_train, X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lrs = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1, 1.2]\n",
    "# for lr in lrs:\n",
    "#     print('lr', lr)\n",
    "#     w, b = singlevar_grad(X_train, Y_train, X_val, Y_val, epochs=100, lr = lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWCVK4LtdJVM"
   },
   "source": [
    "# Multi Variable Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DAQpmHPurOD1"
   },
   "outputs": [],
   "source": [
    "def multivar_grad(X_train, Y_train, X_val, Y_val, epochs=100, lr=1e-3):\n",
    "    '''\n",
    "    Perform multi variable least squares regression using gradient descent\n",
    "\n",
    "    Args:\n",
    "    X_train - numpy array of shape (n_samples_train, 5)\n",
    "    Y_train - numpy array of shape (n_samples_train, 1)\n",
    "    X_val - numpy array of shape (n_samples_val, 5)\n",
    "    Y_val - numpy array of shape (n_samples_val, 1)\n",
    "    epochs - number of gradient descent steps\n",
    "    lr - learnig rate\n",
    "    '''\n",
    "\n",
    "    w = [0.0] * X_train.shape[1]\n",
    "    b = 0\n",
    "\n",
    "    ## TODO\n",
    "    n = len(Y_train)\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        w = np.array(w)\n",
    "        y_pred = np.matmul(X_train, w) + b\n",
    "#         print(y_pred.shape, Y_train.shape, X_train.shape)\n",
    "#         print(np.matmul((y_pred - Y_train), X_train).shape)\n",
    "        dw = 2*(np.matmul((y_pred - Y_train), X_train))/n\n",
    "        db = 2*np.sum(y_pred - Y_train)/n\n",
    "#         print(dw.shape, db.shape)\n",
    "        w -= lr*dw\n",
    "        b -= lr*db\n",
    "\n",
    "    ### END TODO\n",
    "    \n",
    "    mse_train = mse_multi_var(X_train, Y_train, w, b)\n",
    "    mse_val = mse_multi_var(X_val, Y_val, w, b)\n",
    "    print(f'Validation loss if {mse_val}')\n",
    "    print(f'Training Loss loss if {mse_train}')\n",
    "    plot_curves(list(w), b, X_train, Y_train)\n",
    "    return w, b\n",
    "\n",
    "\n",
    "\n",
    "def multivar_closedform(X_train, Y_train, X_val, Y_val):\n",
    "    '''\n",
    "    Perform multi variable least squares regression using closed form \n",
    "    updates\n",
    "\n",
    "    Args:\n",
    "    X_train - numpy array of shape (n_samples_train, 5)\n",
    "    Y_train - numpy array of shape (n_samples_train, 1)\n",
    "    X_val - numpy array of shape (n_samples_val, 5)\n",
    "    Y_val - numpy array of shape (n_samples_val, 1)\n",
    "    '''\n",
    "\n",
    "    w = [0.0] * X_train.shape[1]\n",
    "    b = 0\n",
    "\n",
    "    ## TODO\n",
    "    \n",
    "#     X_train = np.concatenate((X_train, X_val), axis = 0)\n",
    "#     Y_train = np.concatenate((Y_train, Y_val), axis = 0)\n",
    "    \n",
    "    X = np.hstack((X_train, np.ones((X_train.shape[0], 1), dtype=X_train.dtype)))\n",
    "    w_b = np.dot(inv(np.dot(X.T, X)), np.dot(X.T, Y_train))\n",
    "    w = w_b[:-1]\n",
    "    b = w_b[-1]\n",
    "    \n",
    "    print(w, b)\n",
    "    print(X_train.shape)\n",
    "\n",
    "#     w = np.dot(inv(np.dot(X_train.T, X_train)), np.dot(X_train.T, Y_train))\n",
    "#     print(np.dot(X_train.T, X_train).shape)\n",
    "\n",
    "    ## END TODO\n",
    "\n",
    "    mse_train = mse_multi_var(X_train, Y_train, w, b)\n",
    "    mse_val = mse_multi_var(X_val, Y_val, w, b)\n",
    "    print(f'Validation loss if {mse_val}')\n",
    "    print(f'Training Loss loss if {mse_train}')\n",
    "    plot_curves(list(w), b, X_train, Y_train)\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z2yWJ34ou_aF"
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = split_data(X_multi, Y_multi, train_ratio=0.6)\n",
    "\n",
    "w, b = multivar_grad(X_train, Y_train, X_test, Y_test, epochs=1000, lr=0.1)\n",
    "\n",
    "print('----' * 30)\n",
    "\n",
    "w, b = multivar_closedform(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lrs = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1, 1.2]\n",
    "# for lr in lrs:\n",
    "#     print('lr', lr)\n",
    "#     w, b = multivar_grad(X_train, Y_train, X_test, Y_test, epochs=1000, lr = lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sVpDgGodOHX"
   },
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_URsKIPdTXu"
   },
   "outputs": [],
   "source": [
    "\n",
    "def multivar_reg_grad(X_train, Y_train, X_val, Y_val, epochs=100, lr=1e-3, lamda = 1):\n",
    "    '''\n",
    "    Perform L2 regularized multi variable least squares regression using gradient descent\n",
    "\n",
    "    Args:\n",
    "    X_train - numpy array of shape (n_samples_train, 5)\n",
    "    Y_train - numpy array of shape (n_samples_train, 1)\n",
    "    X_val - numpy array of shape (n_samples_val, 5)\n",
    "    Y_val - numpy array of shape (n_samples_val, 1)\n",
    "    epochs - number of gradient descent steps\n",
    "    lr - learnig rate\n",
    "    lamda - regularization weight\n",
    "    '''\n",
    "\n",
    "    w = [0.0] * X_train.shape[1]\n",
    "    b = 0\n",
    "\n",
    "    ## TODO\n",
    "    \n",
    "    n = len(Y_train)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        y_pred = np.dot(X_train, w) + b\n",
    "#         print(y_pred.shape, Y_train.shape, X_train.shape)\n",
    "#         print(np.matmul((y_pred - Y_train), X_train).shape)\n",
    "        \n",
    "        w = np.array(w)\n",
    "#         print(w.shape)\n",
    "        dw = 2*(np.matmul((y_pred - Y_train), X_train))/n + 2*lamda*w\n",
    "        db = 2*np.sum(y_pred - Y_train)/n\n",
    "#         print(dw.shape, db.shape)\n",
    "        w -= lr*dw\n",
    "        b -= lr*db\n",
    "\n",
    "    \n",
    "\n",
    "    ## END TODO\n",
    "\n",
    "    mse_train = mse_regularized(X_train, Y_train, w, b, lamda)\n",
    "    mse_val = mse_regularized(X_val, Y_val, w, b, lamda)\n",
    "    print(f'Validation loss if {mse_val}')\n",
    "    print(f'Training Loss loss if {mse_train}')\n",
    "    plot_curves(list(w), b, X_train, Y_train)\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def multivar_reg_closedform(X_train, Y_train, X_val, Y_val, lamda=0.5):\n",
    "    '''\n",
    "    Perform L2 regularized multi variable least squares regression using \n",
    "    closed form update rules\n",
    "\n",
    "    Args:\n",
    "    X_train - numpy array of shape (n_samples_train, 5)\n",
    "    Y_train - numpy array of shape (n_samples_train, 1)\n",
    "    X_val - numpy array of shape (n_samples_val, 5)\n",
    "    Y_val - numpy array of shape (n_samples_val, 1)\n",
    "    lambda - regularization weight\n",
    "    '''\n",
    "\n",
    "    w = [0.0] * X_train.shape[1]\n",
    "    b = 0\n",
    "\n",
    "    ## TODO\n",
    "    n = len(Y_train)\n",
    "    X = np.hstack((X_train, np.ones((X_train.shape[0], 1), dtype=X_train.dtype)))\n",
    "    w_b = np.dot(inv(np.dot(X.T, X) + n*lamda*np.identity(X.shape[1])), np.dot(X.T, Y_train)) \n",
    "    \n",
    "    w = w_b[:-1]\n",
    "    b = w_b[-1]\n",
    "\n",
    "    ## END TODO\n",
    "\n",
    "    mse_train = mse_regularized(X_train, Y_train, w, b, lamda)\n",
    "    mse_val = mse_regularized(X_val, Y_val, w, b, lamda)\n",
    "    print(f'Validation loss if {mse_val}')\n",
    "    print(f'Training Loss loss if {mse_train}')\n",
    "    plot_curves(list(w), b, X_train, Y_train)\n",
    "    return w, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UxQ47oThrOD2"
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_val, Y_val = split_data(X_multi, Y_multi, train_ratio=0.6)\n",
    "\n",
    "w, b = multivar_reg_grad(X_train, Y_train, X_val, Y_val, lr = 0.05, epochs=1000, lamda=0.001)\n",
    "\n",
    "print('----' * 30)\n",
    "\n",
    "w, b = multivar_reg_closedform(X_train, Y_train, X_val, Y_val, lamda=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lrs = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1, 1.2]\n",
    "# for lr in lrs:\n",
    "#     print('lr', lr)\n",
    "#     w, b = multivar_reg_grad(X_train, Y_train, X_val, Y_val, epochs=1000, lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lamdas = [0.001, 0.002, 0.005, 0.008, 0.01, 0.025, 0.05, 0.08, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1, 1.2, 1.5, 1.6, 2, 2.1, 2.2, 2.5, 3]\n",
    "# for lamda in lamdas:\n",
    "#     print('lambda', lamda)\n",
    "# #     w, b = multivar_reg_grad(X_train, Y_train, X_val, Y_val, epochs=5000, lr = 0.01, lamda = lamda)\n",
    "#     w, b = multivar_reg_closedform(X_train, Y_train, X_val, Y_val, lamda= lamda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdZdA6K5dUGQ"
   },
   "source": [
    "# Bayesian Linear Regression\n",
    "\n",
    "As we studied in class, in Bayesian approach, we model the parameters of the model as a Random variable. \n",
    "\n",
    "We assume a (conjugate) prior distribution over the parameters ($p(w)$) and update  the posterior $p(w | D)$ based on the obervations.\n",
    "\n",
    "In this assignment, we will assume that parameters are sampled from Gaussian distribution and try to learn the mean and variance of posterior.\n",
    "\n",
    "Following the above discussion,\n",
    "\n",
    "- let prior $p(w) = \\mathcal{N}(\\mu_0, \\sum_0) = \\mathcal{N}(0, 0.5)$\n",
    "- We know that $p(w | \\mathcal{D}) \\propto p(w) P(\\mathcal{D} | w)$\n",
    "- Then, $p(w | \\mathcal{D}) = \\mathcal{N}(\\mu_N, \\sum_N)$\n",
    "- And $p(\\mathcal{D} | w) = \\mathcal{N}(y| w^Tx, \\sigma^2I)$\n",
    "\n",
    "For this assignment, feel free to play with various values of $\\sigma^2$ that gives you a better fit.\n",
    "\n",
    "Finally the posterior parameters are given by, <br>\n",
    "$\\mu_N = \\sum_N (\\sum_0^{-1} \\mu_0 + \\frac{1}{\\sigma^2}X^Ty)$ <br>\n",
    "$\\sum_N = (\\sum_0^{-1} + \\frac{1}{\\sigma^2}X^TX)^{-1}$\n",
    "\n",
    "\n",
    "### Learning Bias b\n",
    "Eventhough the above formula shows how to compute $w$, we expect you to learn the bias $b$ also.\n",
    "\n",
    "<font color='blue'> Hint: You can append a constant $1$ to all the input features $x$. Thus making the data $X \\in \\mathcal{R}^{d+1}$. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ae2W0mYFdWRA"
   },
   "outputs": [],
   "source": [
    "def bayesion_lr(X_train, Y_train, X_val, Y_val):\n",
    "    '''\n",
    "    Perform Bayesian Linear Regression\n",
    "\n",
    "    Args:\n",
    "    X_train - numpy array of shape (n_samples_train, 5)\n",
    "    Y_train - numpy array of shape (n_samples_train, 1)\n",
    "    X_val - numpy array of shape (n_samples_val, 5)\n",
    "    Y_val - numpy array of shape (n_samples_val, 1)\n",
    "    '''\n",
    "\n",
    "    ## TODO\n",
    "    sigma_0 = 0.5\n",
    "    mu_0 = 0\n",
    "    X = np.hstack((X_train, np.ones((X_train.shape[0], 1), dtype=X_train.dtype)))\n",
    "#     print(X.shape)\n",
    "    \n",
    "    sigma = 5\n",
    "    \n",
    "    sigma_N = inv(1/sigma_0 + 1/(sigma**2) * np.dot(X.T, X))\n",
    "    mu_N = np.dot(sigma_N, (mu_0/sigma_0 + 1/(sigma**2) * np.dot(X.T, Y_train)))\n",
    "    \n",
    "#     print(sigma_N.shape, mu_N.shape)\n",
    "    \n",
    "    w = list(mu_N[:-1])\n",
    "    b = mu_N[-1]\n",
    "#     print(w, b)\n",
    "    \n",
    "    ## END TODO\n",
    "    \n",
    "    mse_train = mse_multi_var(X_train, Y_train, w, b)\n",
    "    mse_val = mse_multi_var(X_val, Y_val, w, b)\n",
    "    print(f'Validation loss if {mse_val}')\n",
    "    print(f'Training Loss loss if {mse_train}')\n",
    "\n",
    "    plot_curves(w, b, X_train, Y_train)\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zByymfSePAsM"
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_val, Y_val = split_data(X_multi, Y_multi, train_ratio=0.6)\n",
    "\n",
    "bayesion_lr(X_train, Y_train, X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigs = [0.5, 0.75, 0.9, 1, 2, 2.5, 3, 3.5, 4, 5, 8, 10, 20, 30, 40]\n",
    "\n",
    "# for sigma in sigs:\n",
    "#     print(sigma)\n",
    "#     bayesion_lr(X_train, Y_train, X_val, Y_val, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "collapsed_sections": [],
   "name": "polynomial_curve_fitting (Questions).ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "e9d3b97f51c73fb030ee137a5b8f49d0f37ab530bc5cbc39aaffde567a81edd4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
